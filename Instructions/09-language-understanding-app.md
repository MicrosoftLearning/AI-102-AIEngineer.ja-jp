---
lab:
  title: Language Understanding アプリを作成する
  module: Module 5 - Creating Language Understanding Solutions
ms.openlocfilehash: d8a32a2b6404e81d8a5d69cef874fad209de1bfc
ms.sourcegitcommit: d6da3bcb25d1cff0edacd759e75b7608a4694f03
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 11/16/2021
ms.locfileid: "137819407"
---
# <a name="create-a-language-understanding-app"></a>Language Understanding アプリを作成する

Language Understanding サービスを使用すると、アプリケーションがユーザーからの自然言語入力を解釈し、ユーザーの *意図* (達成したいこと) を予測し、意図を適用する必要がある *エンティティ* を特定するために使用できる言語モデルをカプセル化するアプリを定義できます。

たとえば、時計アプリケーション用の言語理解アプリは、次のような入力を処理することが期待される場合があります。

*What's the time in London?* (ロンドンの時刻は何時ですか?)

この種の入力は、*発話* (ユーザーが言うまたは入力する可能性のあるもの) の例です。*意図* は、特定の場所 (*エンティティ*) (この場合はロンドン) の時刻を得ることです。

> **注**: 言語理解アプリのタスクは、ユーザーの意図を予測し、意図が適用されるエンティティを特定することです。 意図を満たすために必要なアクションを実際に実行することは、その仕事では<u>ありません</u>。 たとえば、時計アプリケーションは言語アプリを使用して、ユーザーがロンドンの時刻を知りたいことを識別できます。ただし、クライアント アプリケーション自体は、正しい時刻を決定してユーザーに提示するロジックを実装する必要があります。

## <a name="clone-the-repository-for-this-course"></a>このコースのリポジトリを複製する

このラボで作業している環境に **AI-102-AIEngineer** コードのリポジトリをまだクローンしていない場合は、次の手順に従ってクローンします。 それ以外の場合は、複製されたフォルダーを Visual Studio Code で開きます。

1. Visual Studio Code を起動します。
2. パレットを開き (Shift + Ctrl + P)、**Git: Clone** コマンドを実行して、`https://github.com/MicrosoftLearning/AI-102-AIEngineer` リポジトリをローカル フォルダーにクローンします (どのフォルダーでも問題ありません)。
3. リポジトリを複製したら、Visual Studio Code でフォルダーを開きます。
4. リポジトリ内の C# コード プロジェクトをサポートするために追加のファイルがインストールされるまで待ちます。

    > **注**: ビルドとデバッグに必要なアセットを追加するように求めるダイアログが表示された場合は、 **[今はしない]** を選択します。

## <a name="create-language-understanding-resources"></a>Language Understanding リソースの作成

Language Understanding サービスを使用するには、次の 2 種類のリソースが必要です。

- *オーサリング* リソース: Language Understanding アプリの定義、トレーニング、およびテストに使用されます。 Azure サブスクリプションの **Language Understanding - オーサリング** リソースである必要があります。
- *予測* リソース: Language Understanding アプリを公開し、それを使用するクライアント アプリケーションからのリクエストを処理するために使用されます。 Azure サブスクリプションの **Language Understanding** リソースまたは **Cognitive Services** リソースのどちらでも構いません。

     > **重要**: オーサリング リソースは、3 つの *リージョン* (ヨーロッパ、オーストラリア、または米国) のいずれかで作成する必要があります。 ヨーロッパまたはオーストラリアのオーサリング リソースで作成された Language Understanding アプリは、それぞれヨーロッパまたはオーストラリアの予測リソースにのみデプロイできます。米国のオーサリング リソースで作成されたモデルは、ヨーロッパとオーストラリア以外の Azure の場所にある予測リソースにデプロイできます。 一致するオーサリングと予測の場所の詳細については、[オーサリングとパブリッシング リージョンのドキュメント](https://docs.microsoft.com/azure/cognitive-services/luis/luis-reference-regions)を参照してください。

言語理解のオーサリングおよび予測リソースをまだ持っていない場合：

1. Azure portal (`https://portal.azure.com`) を開き、ご利用の Azure サブスクリプションに関連付けられている Microsoft アカウントを使用してサインインします。
2. **[&#65291;リソースの作成]** ボタンを選択して、「*language understanding*」を検索し、次の設定を使用して **Language Understanding** リソースを作成します。

    *Language Understanding (Azure Cognitive Services) では <u>なく</u>、**Language Understanding** を選択していることを確認してください*

    - **[Create option]\(作成オプション\)**: 両方
    - **[サブスクリプション]**:"*ご自身の Azure サブスクリプション*"
    - **リソース グループ**: *リソース グループを選択または作成します (制限付きサブスクリプションを使用している場合は、新しいリソース グループを作成する権限がないことがあります。提供されているものを使ってください)*
    - **[名前]**: *一意の名前を入力します*
    - **[作成場所]**: *希望の場所を選択します*
    - **[価格レベルを作成しています]**: F0
    - **予測の場所**: *作成場所と同じです*
    - **[予測価格レベル]**: F0
3. リソースが作成されるまで待ちます。2 つの Language Understanding リソースがプロビジョニングされていることに注意してください。1 つは作成用、もう 1 つは予測用です。 作成先のリソース グループに移動すると、この両方を表示できます。 **[リソースに移動]** を選択すると、*オーサリング* リソースが開きます。

## <a name="create-a-language-understanding-app"></a>Language Understanding アプリを作成する

オーサリング リソースを作成したので、それを使用して Language Understanding アプリを作成できます。

1. ブラウザーの新しいタブで、`https://www.luis.ai` の Language Understanding ポータルを開きます。
2. Azure サブスクリプションに関連付けられている Microsoft アカウントを使用してサインインします。 初めて Language Understanding ポータルにサインインする場合は、アカウントの詳細にアクセスするために、アプリケーションにいくつかのアクセス許可を付与する必要が生じることがあります。 次に、Azure サブスクリプションと作成したオーサリング リソースを選択して、*ようこそ* の手順を完了します。

    > **注**: アカウントが異なるディレクトリ内の複数のサブスクリプションに関連付けられている場合は、Language Understanding リソースをプロビジョニングしたサブスクリプションを含むディレクトリに切り替える必要がある場合があります。

3. **[会話アプリ]** ページで、サブスクリプションと Language Understanding オーサリング リソースが選択されていることを確認します。 次に、次の設定で会話用の新しいアプリを作成します。
    - **名前**: Clock
    - **カルチャ**: 英語 (*このオプションが使用できない場合は空白のままにします*)
    - **説明**: Natural language clock (自然言語の時計)
    - **予測リソース**: *Language Understanding 予測リソース*

    **Clock** アプリが自動的に開かない場合は、それを開いてください。
    
    効果的な Language Understanding アプリを作成するためのヒントが表示されたパネルが表示された場合、そのパネルを閉じます。

## <a name="create-intents"></a>意図の作成

新しいアプリで最初に行うことは、いくつかの意図を定義することです。

1. **[意図]** ページで、 **[&#65291; 作成]** を選択し、**GetTime** という名前の新しい意図を作成します。
2. **GetTime** 意図で、ユーザー入力の例として次の発話を追加します。

    *what is the time?* (時刻は何時ですか)

    *what time is it?* (何時ですか)

3. これらの発話を追加したら、 **[意図]** ページに戻り、次の発話を含む **GetDay** という名前の別の新しい意図を追加します。

    *what is the day today?* (今日の曜日は何ですか)

    *what day is it?* (今日は何曜日ですか)

4. これらの発話を追加したら、 **[意図]** ページに戻り、次の発話を含む **GetDate** という名前の別の新しい意図を追加します。

    *what is the date today?* (今日の日付は何ですか)

    *what date is it?* (今日は何日ですか)

5. これらの発話を追加したら、 **[意図]** ページに戻り、**None** 意図を選択します。 これは、言語モデルで定義した意図のいずれにもマップされない入力のフォールバックとして提供されます。
6. 次の発話を **None** 意図に追加します。

    *hello*

    *goodbye*

## <a name="train-and-test-the-app"></a>アプリの訓練およびテスト

意図をいくつか追加したので、アプリをトレーニングして、ユーザー入力から正しく予測できるかどうかを確認しましょう。

1. ポータルの右上で、 **[トレーニング]** を選択してアプリをトレーニングします。
2. アプリがトレーニングされたら、 **[テスト]** を選択して [テスト] パネルを表示し、次のテスト発話を入力します。

    *what's the time now?* (今何時ですか)

    返された結果を確認します。予測された意図 (**GetTime** であるはずです) と、予測された意図に対してモデルが計算した確率を示す信頼スコアが含まれていることに注意してください。

3. 次のテスト発話を試してください。

    *tell me the time* (時刻を教えて下さ)

    もう一度、予測された意図と信頼スコアを確認します。

4. 次のテスト発話を試してください。

    *what's today?* (今日は何曜日ですか)

    うまくいけば、モデルは **GetDay** 意図を予測します。

5. 最後に、このテスト発話を試してください。

    *hi*

    これにより、**None** 意図が返されます。

6. [テスト] パネルを閉じます。

## <a name="add-entities"></a>複数エンティティの追加

これまで、意図にマップするいくつかの簡単な発話を定義しました。 ほとんどの実際のアプリケーションには、より複雑な発話が含まれており、意図のコンテキストを増やすために、特定のデータ エンティティを抽出する必要があります。

### <a name="add-a-machine-learned-entity"></a>*機械学習* エンティティを追加する

最も一般的な種類のエンティティは *機械学習エンティティ* であり、アプリは例に基づいてエンティティ値を識別することを学習します。

1. **[エンティティ]** ページで、 **[&#65291; 作成]** を選択し、新しいエンティティを作成します。
2. **[エンティティの作成]** ダイアログ ボックスで、**Location** という名前の **機械学習** エンティティを作成します。
3. **Location** エンティティが作成されたら、 **[意図]** ページに戻り、**GetTime** 意図を選択します。
4. 次の新しい発話例を入力します。

    *what time is it in London?* (ロンドンの時刻は何時ですか)

5. 発話が追加されたら、***london** _ という単語を選び、表示されるドロップダウン リストで _ *Location** を選んで、"london" が場所の例であることを示します。
6. 別の発話例を追加します。

    *what is the current time in New York?* (ニューヨークの時刻は何時ですか)

7. 発話が追加されたら、***new york** _ という単語を選び、それらを _ *Location** エンティティにマップします。

### <a name="add-a-list-entity"></a>*リスト* エンティティを追加する

場合によっては、エンティティの有効な値を特定の用語と同義語のリストに制限できます。これは、アプリが発話内のエンティティのインスタンスを識別するのに役立ちます。

1. **[エンティティ]** ページで、 **[&#65291; 作成]** を選択し、新しいエンティティを作成します。
2. **[エンティティの作成]** ダイアログボックスで、**Weekday** という名前の **リスト** エンティティを作成します。
3. 次の **正規化** された値と **シノニム** を追加します。

    | 正規化された値 | シノニム|
    |-------------------|---------|
    | sunday | sun |
    | monday | mon |
    | tuesday | tue |
    | wednesday | wed |
    | thursday | thu |
    | friday | fri |
    | saturday | sat |

3. **Weekday** エンティティが作成されたら、 **[意図]** ページに戻り、**GetDate** 意図を選択します。
4. 次の新しい発話例を入力します。

    *what date was it on Saturday?* (土曜日は何日でしたか)

5. 発話が追加されたら、**saturday** が **Weekday** エンティティに自動的にマップされていることを確認します。 そうでない場合は、**_saturday_ *_ という単語を選択し、表示されるドロップダウン リストで _* [Weekday]** を選択します。
6. 別の発話例を追加します。

    *what date will it be on Friday?* (金曜日は何日ですか)

7. 発話が追加されたら、**friday** が **Weekday** エンティティにマップされていることを確認します。

### <a name="add-a-regex-entity"></a>*正規表現エ* ンティティを追加する

エンティティには、シリアル番号、フォームコード、日付などの特定の形式がある場合があります。 アプリが一致するエンティティ値を識別するのに役立つ、予想される形式を説明する正規表現 (*regex*) を定義できます。

1. **[エンティティ]** ページで、 **[&#65291; 作成]** を選択し、新しいエンティティを作成します。
2. **[エンティティの作成]** ダイアログ ボックスで、次の正規表現を使用して **Date** という名前の **正規表現** エンティティを作成します。

    ```
    [0-9]{2}/[0-9]{2}/[0-9]{4}
    ```

    > **注**: これは単純な正規表現です。これは、2 桁の後に "/"、別の 2 桁、別の "/"、および 4 桁が続くかどうかをチェックします (たとえば、*01/11/2020*)。 *56/00/9999* などの無効な日付を許可します。ただし、エンティティ正規表現は、日付値を検証するためではなく、日付として *意図された* データ入力を識別するために使用されることを覚えておくことが重要です。

3. **Date** エンティティが作成されたら、 **[意図]** ページに戻り、**GetDay** 意図を選択します。
4. 次の新しい発話例を入力します。

    *what day was 01/01/1901?* (1901 年 1 月 1 日は何曜日でしたか)

5. 発話が追加されたら、**01/01/1901** が自動的に **Date** エンティティにマップされていることを確認します。 そうでない場合は、**_01/01/1901_ *_ を選択し、表示されるドロップダウン リストで _* [日付]** を選択します。
6. 別の発話例を追加します。

    *what day will it be on 12/12/2099?* (2099 年 12 月 12 日は何曜日ですか)

7. 発話が追加されたら、**12/12/2099** が **Date** エンティティにマップされていることを確認します。

### <a name="retrain-the-app"></a>アプリを再トレーニングする

言語モデルを変更したので、アプリを再トレーニングして再テストする必要があります。

1. ポータルの右上で、 **[トレーニング]** を選択してアプリを再トレーニングします。
2. アプリがトレーニングされたら、 **[テスト]** を選択して [テスト] パネルを表示し、次のテスト発話を入力します。

    *what's the time in Edinburgh?* (エジンバラの時刻は何時ですか)

3. 返される結果を確認します。これにより、**GetTime** の意図が予測されるはずです。 次に、 **[検査]** を選択し、表示される追加の検査パネルで、 **[ML エンティティ]** セクションを調べます。 モデルは、"edinburgh" が **Location** エンティティのインスタンスであると予測するはずです。
4. 次の発話をテストしてみてください。

    *what date is it on Friday?* (金曜日は何日ですか)

    *what's the date on Thu?* (木曜日は何日ですか)

    *what was the day on 01/01/2020?* (2020 年 1 月 1 日は何曜日ですか)

5. テストが終了したら、検査パネルを閉じますが、テスト パネルは開いたままにしておきます。

## <a name="perform-batch-testing"></a>バッチ テストを実行する

テスト ペインを使用して個々の発話をインタラクティブにテストできますが、より複雑な言語モデルの場合は、通常、*バッチ テスト* を実行する方が効率的です。

1. Visual Studio Code で、**09-luis-app** フォルダーにある **batch-test.json** ファイルを開きます。 このファイルは、作成した時計言語モデルの複数のテストケースを含む JSON ドキュメントで構成されています。
2. Language Understanding ポータルの[テスト] パネルで、 **[バッチテスト] パネル** を選択します。 次に、 **[&#65291; インポート]** を選択し、**batch-test.json** ファイルをインポートして、**clock-test** という名前を割り当てます。
3. [バッチ テスト] パネルで、**clock-test** テストを実行します。
4. テストが完了したら、 **[結果を表示]** を選択します。
5. 結果ページで、予測結果を表す混同行列を表示します。 右側のリストで選択されている意図またはエンティティの真陽性、偽陽性、真陰性、および偽陰性の予測が表示されます。

    ![Language Understanding バッチ テストの混同行列](./images/luis-confusion-matrix.jpg)

    > **注**: 各発話は、各意図について *肯定的* または *否定的* としてスコアリングされます。たとえば、"what time is it?" は、 **GetTime** の意図については *肯定的*、**GetDate** の意図については *否定的* としてスコアリングされるはずです。 混同行列上のポイントは、選択した意図について、どの発話が *肯定的* および *否定的* として正しく予測されたか (*true*)、誤って予測されたか (*false*) を示します。

6. **GetDate** 意図を選択した状態で、混同行列上の任意のポイントを選択して、予測の詳細 (発話や予測の信頼度スコアなど) を確認します。 次に、**GetDay**、**GetTime**、および **None** の意図を選択し、それらの予測結果を表示します。 アプリは、意図を正しく予測できるように、うまく機能しているはずです。

    > **注**: ユーザー インターフェイスでは、以前に選択したポイントがクリアされない場合があります。

7. **Location** エンティティを選択し、混同行列で予測結果を表示します。 具体的には、*偽陰性* だった予測に注意してください。これらは、アプリが発話内の指定された場所を検出できなかった場合であり、その意図にさらにサンプル発話を追加し、モデルを再トレーニングする必要があることを示しています。
8. [バッチ テスト] パネルを閉じます。

## <a name="publish-the-app"></a>アプリの発行

実際のプロジェクトでは、予測パフォーマンスに満足するまで、意図とエンティティを繰り返し改良し、再トレーニングして、再テストします。 次に、クライアント アプリケーションが使用できるようにアプリを公開できます。

1. Language Understanding ポータルの右上にある **[公開]** を選択します。
2. **[本番スロット]** を選択し、アプリを公開します。
3. 公開が完了したら、Language Understanding ポータルの上部にある **[管理]** を選択します。
4. **[設定]** ページで、**アプリ ID** をメモします。 クライアント アプリケーションがアプリを使用するには、これが必要です。
5. **[Azure リソース]** ページで、アプリを使用できる予測リソースの **プライマリ キー**、**セカンダリ キー**、**エンドポイント URL** をメモします。 クライアント アプリケーションは、予測リソースに接続して認証されるために、エンドポイントとキーの 1 つを必要とします。
6. Visual Studio Code の **09-luis-app** フォルダーで、**GetIntent.cmd** バッチ ファイルを選択し、そこに含まれるコードを表示します。 このコマンドライン スクリプトは、cURL を使用して、指定されたアプリケーションと予測エンドポイントの Language Understanding REST API を呼び出します。
7. スクリプト内のプレースホルダー値を、**アプリ ID**、**エンドポイント URL**、Language Understanding アプリの **プライマリ キー** または **セカンダリ キー** のいずれかに置き換えます。次に、更新したファイルを保存します。
8. **09-luis-app** フォルダーを右クリックして、統合ターミナルを開きます。 次に、次のコマンドを入力します (必ず引用符を含めてください)。

    ```
    GetIntent "What's the time?"
    ```

9. アプリから返された JSON 応答を確認します。これは、入力に対して予測された最高スコアの意図 (**GetTime** のはずです) が表示されます。
10. 次のコマンドをお試しください。

    ```
    GetIntent "What's today's date?"
    ```

11. 応答を調べて、**GetDate** 意図を予測していることを確認します。
12. 次のコマンドをお試しください。

    ```
    GetIntent "What time is it in Sydney?"
    ```

13. 応答を調べて、**Location** エンティティが含まれていることを確認します。

14. 次のコマンドを試して、応答を調べてください。

    ```
    GetIntent "What time is it in Glasgow?"
    ```

    ```
    GetIntent "What's the time in Nairobi?"
    ```

    ```
    GetIntent "What's the UK time?"
    ```
15. さらにいくつかのバリエーションを試してください。目標は、**GetTime** 意図を正しく予測するが、**Location** エンティティの検出に失敗する少なくともいくつかの応答を生成することです。

    ターミナルを開いたままにします。 後で戻ります。

## <a name="apply-active-learning"></a>*アクティブ ラーニング* を適用する

エンドポイントに送信された過去の発話に基づいて、Language Understanding アプリを改善できます。 この実践は *アクティブ ラーニング* と呼ばれます。

前の手順では、cURL を使用してアプリのエンドポイントにリクエストを送信しました。 これらのリクエストには、クエリをログに記録するオプションが含まれていました。これにより、アプリはアクティブ ラーニングで使用するためにクエリを追跡できます。

1. Language Understanding ポータルで、 **[ビルド]** を選択し、 **[エンドポイントの発話の確認]** ページを表示します。 このページには、サービスがレビュー用にフラグを付けたログに記録された発話が一覧表示されます。
2. 意図と新しい Location エンティティ (元のトレーニング発話に含まれていなかった) が正しく予測された発話については、 **&#10003;** を選択してエンティティを確認し、 **&#10514;** アイコンを使用して、トレーニングの例として、発話を意図に追加します。
3. **GetTime** 意図が正しく識別されたが、**Location** エンティティが識別され <u>なかった</u>発話の例を見つけます。location 名を選択して、**location** エンティティにマップします。 次に、 **&#10514;** アイコンを使用して、トレーニングの例として発話を意図に追加します。
4. **[意図]** ページに移動し、**GetTime** 意図を開いて、提案された発話が追加されたことを確認します。
5. Language Understanding ポータルの上部にある **[トレーニング]** を選択して、アプリを再トレーニングします。
6. Language Understanding ポータルの右上にある **[公開]** を選択し、アプリを **本番スロット** に再公開します。
7. **09-luis-app** フォルダーのターミナルに戻り、**GetIntent** コマンドを使用して、アクティブ ラーニング中に追加および修正した発話を送信します。
8. 結果に **Location** エンティティが含まれていることを確認します。 次に、同じ言い回しを使用しているが、別の場所を指定している別の発話を試してください (たとえば、*Berlin*)。

## <a name="export-the-app"></a>アプリをエクスポートする

Language Understanding ポータルを使用して言語アプリを開発およびテストできますが、DevOps のソフトウェア開発プロセスでは、継続的インテグレーションとデリバリー (CI/CD) パイプラインに含めることができるアプリのソース制御定義を維持する必要があります。 コード スクリプトで Language Understanding SDK または REST API を使用してアプリを作成およびトレーニング *できます* が、より簡単な方法は、ポータルを使用してアプリを作成し、それを *.lu* ファイルとしてエクスポートして、別の言語理解インスタンスにインポートして再トレーニングできるようにすることです。 このアプローチにより、アプリの移植性と再現性を維持しながら、ポータルの生産性のメリットを活用できます。

1. Language Understanding ポータルで、 **[管理]** を選択します。
2. **[バージョン]** ページで、アプリの現在のバージョンを選択します (1 つだけである必要があります)。
3. **[エクスポート]** ドロップダウン リストで、 **[LU としてエクスポート]** を選択します。 次に、ブラウザーのプロンプトが表示されたら、ファイルを **09-luis-app** フォルダーに保存します。
4. Visual Studio Code で、エクスポートしてダウンロードした **.lu** ファイルを開きます (マーケットプレイスでそれを読み取ることができる拡張機能を検索するように求められた場合は、プロンプトを閉じます)。 LU 形式は人間が読める形式であるため、チーム開発環境で Language Understanding アプリの定義を文書化する効果的な方法であることに注意してください。

## <a name="more-information"></a>詳細情報

**Language Understanding** サービスの使用の詳細については、[Language Understanding のドキュメント](https://docs.microsoft.com/azure/cognitive-services/luis/)を参照してください。
